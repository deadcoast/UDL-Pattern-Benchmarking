# ğŸ•°ï¸ The Continuous Thought Machine

ğŸ“š [PAPER: Technical Report](https://arxiv.org/abs/2505.05522) | ğŸ“ [Blog](https://sakana.ai/ctm/) | ğŸ•¹ï¸ [Interactive Website](https://pub.sakana.ai/ctm) | âœï¸ [Tutorial](examples/01_mnist.ipynb)

![Activations](assets/activations.gif)

We present the Continuous Thought Machine (CTM), a model designed to unfold and then leverage neural activity as the underlying mechanism for observation and action. Our contributions are:

1. An internal temporal axis, decoupled from any input data, that enables neuron activity to unfold.

2. Neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals, enabling fine-grained temporal dynamics.

3. Neural synchronisation, employed as a direct latent representation for modulating data and producing outputs, thus directly encoding information in the timing of neural activity.

We demonstrate the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks.

We provide all necessary code to reproduce our results and invite others to build upon and use CTMs in their own work.

## [Interactive Website](https://pub.sakana.ai/ctm)
Please see our [Interactive Website](https://pub.sakana.ai/ctm) for a maze-solving demo, many demonstrative videos of the method, results, and other findings. 


## Repo structure
```
â”œâ”€â”€ tasks
â”‚Â Â  â”œâ”€â”€ image_classification
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ train.py                          # Training code for image classification (cifar, imagenet)
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ imagenet_classes.py               # Helper for imagenet class names
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plotting.py                       # Plotting utils specific to this task
â”‚Â Â  â”‚Â Â  â””â”€â”€ analysis
â”‚Â Â  â”‚Â Â      â”œâ”€â”€run_imagenet_analysis.py       # ImageNet eval and visualisation code
â”‚Â Â  â”‚Â Â   Â Â  â””â”€â”€outputs/                       # Folder for outputs of analysis
â”‚Â Â  â”œâ”€â”€ mazes
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ train.py                          # Training code for solving 2D mazes (by way of a route; see paper)
â”‚Â Â  â”‚Â Â  â””â”€â”€ plotting.py                       # Plotting utils specific to this task
â”‚Â Â  â”‚Â Â  â””â”€â”€ analysis
â”‚Â Â  â”‚Â Â      â”œâ”€â”€run.py                         # Maze analysis code
â”‚Â Â  â”‚Â Â   Â Â  â””â”€â”€outputs/                       # Folder for outputs of analysis
â”‚Â Â  â”œâ”€â”€ sort
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ train.py                          # Training code for sorting
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.py                          # Sort specific utils (e.g., CTC decode)
â”‚Â Â  â”œâ”€â”€ parity
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ train.py                          # Training code for parity task
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ utils.py                          # Parity-specific helper functions
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plotting.py                       # Plotting utils specific to this task
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scripts/
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ *.sh                          # Training scripts for different experimental setups
â”‚Â Â  â”‚Â Â  â””â”€â”€ analysis/
â”‚Â Â  â”‚Â Â      â””â”€â”€ run.py                        # Entry point for parity analysis
â”‚Â Â  â”œâ”€â”€ qamnist
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ train.py                          # Training code for QAMNIST task (quantized MNIST)
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ utils.py                          # QAMNIST-specific helper functions
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plotting.py                       # Plotting utils specific to this task
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scripts/
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ *.sh                          # Training scripts for different experimental setups
â”‚Â Â  â”‚Â Â  â””â”€â”€ analysis/
â”‚Â Â  â”‚Â Â      â””â”€â”€ run.py                        # Entry point for QAMNIST analysis
â”‚Â Â  â””â”€â”€ rl
â”‚Â Â   Â Â  â”œâ”€â”€ train.py                          # Training code for RL environments
â”‚Â Â   Â Â  â”œâ”€â”€ utils.py                          # RL-specific helper functions
â”‚Â Â   Â Â  â”œâ”€â”€ plotting.py                       # Plotting utils specific to this task
â”‚Â Â   Â Â  â”œâ”€â”€ envs.py                           # Custom RL environment wrappers
â”‚Â Â   Â Â  â”œâ”€â”€ scripts/
â”‚Â Â   Â Â  â”‚Â Â  â”œâ”€â”€ 4rooms/
â”‚Â Â   Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ *.sh                      # Training scripts for MiniGrid-FourRooms-v0 environment
â”‚Â Â   Â Â  â”‚Â Â  â”œâ”€â”€ acrobot/
â”‚Â Â   Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ *.sh                      # Training scripts for Acrobot-v1 environment
â”‚Â Â   Â Â  â”‚Â Â  â””â”€â”€ cartpole/
â”‚Â Â   Â Â  â”‚Â Â      â””â”€â”€ *.sh                      # Training scripts for CartPole-v1 environment
â”‚Â Â   Â Â  â””â”€â”€ analysis/
â”‚Â Â   Â Â      â””â”€â”€ run.py                        # Entry point for RL analysis
â”œâ”€â”€ data                                      # This is where data will be saved and downloaded to
â”‚Â Â  â””â”€â”€ custom_datasets.py                    # Custom datasets (e.g., Mazes), sort
â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ ctm.py                                # Main model code, used for: image classification, solving mazes, sort
â”‚Â Â  â”œâ”€â”€ ctm_*.py                              # Other model code, standalone adjustments for other tasks
â”‚Â Â  â”œâ”€â”€ ff.py                                 # feed-forward (simple) baseline code (e.g., for image classification)
â”‚Â Â  â”œâ”€â”€ lstm.py                               # LSTM baseline code (e.g., for image classification)
â”‚Â Â  â”œâ”€â”€ lstm_*.py                              # Other baseline code, standalone adjustments for other tasks
â”‚Â Â  â”œâ”€â”€ modules.py                            # Helper modules, including Neuron-level models and the Synapse UNET
â”‚Â Â  â”œâ”€â”€ utils.py                              # Helper functions (e.g., synch decay)
â”‚Â Â  â””â”€â”€ resnet.py                             # Wrapper for ResNet featuriser
â”œâ”€â”€ utils
â”‚Â Â  â”œâ”€â”€ housekeeping.py                       # Helper functions for keeping things neat
â”‚Â Â  â”œâ”€â”€ losses.py                             # Loss functions for various tasks (mostly with reshaping stuff)
â”‚Â Â  â””â”€â”€ schedulers.py                         # Helper wrappers for learning rate schedulers
â””â”€â”€ checkpoints
 Â Â  â””â”€â”€ imagenet, mazes, ...                  # Checkpoint directories (see google drive link for files)

```

## Setup
To set up the environment using conda:

```
conda create --name=ctm python=3.12
conda activate ctm
pip install -r requirements.txt
```

If there are issues with PyTorch versions, the following can be ran:
```
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

## Model training
Each task has its own (set of) training code. See for instance [tasks/image_classification/train.py](tasks/image_classification/train.py). We have set it up like this to ensure ease-of-use as opposed to clinical efficiency. This code is for researchers and we hope to have it shared in a way that fosters collaboration and learning. 

While we have provided reasonable defaults in the argparsers of each training setup, scripts to replicate the setups in the paper will typically be found in the accompanying script folders. If you simply want to dive in, run the following as a module (setup like this to make it easy to run many high-level training scripts from the top directory):

```
python -m tasks.image_classification.train
```
For debugging in VSCode, this configuration example might be helpful to you:
```
{
    "name": "Debug: train image classifier",
    "type": "debugpy",
    "request": "launch",
    "module": "tasks.image_classification.train",
    "console": "integratedTerminal",
    "justMyCode": false
}
```


## Running analyses

We also provide analysis and plotting code to replicate many of the plots in our paper. See `tasks/.../analysis/*` for more details on that. We also provide some data (e.g., the mazes we generated for training) and checkpoints (see [here](#checkpoints-and-data)). Note that ffmpeg is required for generating mp4 files from the analysis scripts. It can be installed with:
```
conda install -c conda-forge ffmpeg
```


## Checkpoints and data
You can download the data and checkpoints from here: 
- checkpoints: https://drive.google.com/drive/folders/1vSg8T7FqP-guMDk1LU7_jZaQtXFP9sZg
- maze data: https://drive.google.com/file/d/1cBgqhaUUtsrll8-o2VY42hPpyBcfFv86/view?usp=drivesdk

Checkpoints go in the `checkpoints` folder. For instance, when properly populated, the checkpoints folder will have the maze checkpoint in `checkpoints/mazes/...`

## UDL Rating Framework Integration

The UDL Rating Framework integrates with CTM through the `udl_rating_framework/models/ctm_adapter.py` module. This adapter enables neural approximation for fast UDL quality inference.

### Integration Architecture

The CTM adapter provides:

1. **UDLRatingCTM**: A PyTorch module that adapts CTM for UDL quality prediction
   - Token embedding layer for UDL text processing
   - Integration with `ContinuousThoughtMachine` from `models/ctm.py`
   - Rating head that maps synchronization to quality scores in [0,1]

2. **UDLTokenVocabulary**: Vocabulary management for UDL tokens
   - Maps tokens to integer indices for embedding lookup
   - Supports special tokens: `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`

3. **TrackingData**: Container for CTM internal representations
   - Pre/post activations, synchronization data, attention weights
   - HDF5 serialization for analysis and visualization
   - Activation statistics and synchronization evolution metrics

### Usage Example

```python
from udl_rating_framework.models.ctm_adapter import UDLRatingCTM, UDLTokenVocabulary, create_udl_rating_model
from udl_rating_framework.core.representation import UDLRepresentation

# Create vocabulary and model
vocab = UDLTokenVocabulary()
udl = UDLRepresentation(udl_content, "example.udl")
vocab.add_tokens_from_udl(udl)

model = create_udl_rating_model(vocab_size=len(vocab))

# Tokenize and rate
token_ids = model.tokenize_udl(udl, vocab)
ratings, certainties, synch_out, _ = model(token_ids.unsqueeze(0))
print(f"Quality rating: {ratings.item():.3f}")
```

### Key Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `d_model` | 256 | Core dimensionality of CTM latent space |
| `d_input` | 64 | Dimensionality of projected attention outputs |
| `iterations` | 20 | Number of internal 'thought' ticks |
| `n_synch_out` | 32 | Number of neurons for output synchronization |
| `heads` | 8 | Number of attention heads |
| `memory_length` | 10 | History length for Neuron-Level Models |

### Mathematical Definition

Given a UDL represented as token sequence (tâ‚, tâ‚‚, ..., tâ‚™):

1. **Embed**: xáµ¢ = E(táµ¢) âˆˆ â„áµˆ
2. **Process**: S(T) = CTM(xâ‚, xâ‚‚, ..., xâ‚™)
3. **Rate**: q = Ïƒ(WÂ·S(T) + b) âˆˆ [0,1]

The model leverages CTM's temporal processing and neural synchronization to produce quality ratings with associated certainty scores.
